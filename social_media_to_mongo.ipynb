{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reddit scraper\n",
    "# Original source: https://www.geeksforgeeks.org/scraping-reddit-using-python/\n",
    "# Edited by Mark\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connect to Reddit API using PRAW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "my_client_id = '633UnblFcVgTtdNNS-hQUA'\n",
    "my_client_secret = 'h2phjjUTg5EkhNqob_PV5-2BU7iENA'\n",
    "my_user_agent = \"DAP_scrape\"\n",
    "#auth = requests.auth.HTTPBasicAuth(app_id, secret)\n",
    "reddit_username = 'McNData'\n",
    "reddit_password = 'NJdVr7hDXsAfgwEbUnfs'\n",
    "\n",
    "reddit1 = praw.Reddit(\n",
    "    client_id=\"633UnblFcVgTtdNNS-hQUA\",\n",
    "    client_secret=\"h2phjjUTg5EkhNqob_PV5-2BU7iENA\",\n",
    "    password=\"NJdVr7hDXsAfgwEbUnfs\",\n",
    "    user_agent=\"DAP_scrape\",\n",
    "    username=\"McNData\")\n",
    "subred = \"WallStreetBets\"\n",
    "subreddit = reddit1.subreddit(subred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scrape WallStreetBets subreddit thread titles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "import re\n",
    "# Display the name of the Subreddit\n",
    "#print(\"Display Name:\", subreddit.display_name)\n",
    "\n",
    "# Display the title of the Subreddit\n",
    "#print(\"Title:\", subreddit.title)\n",
    "\n",
    "# Display the description of the Subreddit\n",
    "#print(\"Description:\", subreddit.description)\n",
    "\n",
    "\n",
    "#subreddit = reddit1.subreddit(\"WallStreetBets\")\n",
    "\n",
    "#for post in subreddit.hot(limit=100):\n",
    " #   title = post.title.encode('utf-8')\n",
    "  #  print(title)\n",
    "   # print()\n",
    "\n",
    "\n",
    "posts = reddit1.subreddit(\"wallstreetbets\")\n",
    "# Scraping the top posts of the current month\n",
    "\n",
    "#posts_dict = {\"Title\": [], \"Post Text\": [],\n",
    "#              \"ID\": [], \"Score\": [],\n",
    "#              \"Total Comments\": [] #\"Post URL\": []\n",
    "#              }\n",
    "posts_data = []\n",
    "for post in posts.search((\"elon musk\"), limit=1000):\n",
    "    #if \"twitter\" in post.title:\n",
    "        # Title of each post\n",
    "    posts_data.append([post.title,int(post.created), post.selftext,post.id,post.score,post.num_comments])\n",
    "\n",
    "\n",
    "# Convert Epoch time to datetime\n",
    "reddit_df = pd.DataFrame(posts_data, columns = ['title','created','body', 'id','score','num_comments'])\n",
    "# Filter for posts only with Twitter in title\n",
    "reddit_df = reddit_df[reddit_df[\"title\"].str.contains(\"Twitter\") == True]\n",
    "\n",
    "\n",
    "reddit_df['created'] = pd.to_datetime(reddit_df['created'], unit='s')\n",
    "# reset the index\n",
    "reddit_df = reddit_df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert Epoch time in comments to UTC datetime\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "\n",
    "reddit_df['created'] = pd.to_datetime(reddit_df['created'], unit='s')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[66], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m reddit_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m reddit_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m reddit_df:\n\u001B[1;32m----> 3\u001B[0m     reddit_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m][i] \u001B[38;5;241m=\u001B[39m \u001B[43mreddit_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbody\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mstrip()\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\core\\series.py:981\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    978\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[0;32m    980\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[1;32m--> 981\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_hashable(key):\n\u001B[0;32m    984\u001B[0m     \u001B[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001B[39;00m\n\u001B[0;32m    985\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    986\u001B[0m         \u001B[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\core\\series.py:1089\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[1;34m(self, label, takeable)\u001B[0m\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[0;32m   1088\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[1;32m-> 1089\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1090\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39m_get_values_for_loc(\u001B[38;5;28mself\u001B[39m, loc, label)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\core\\indexes\\range.py:395\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m    393\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    394\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\u001B[1;32m--> 395\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mget_loc(key, method\u001B[38;5;241m=\u001B[39mmethod, tolerance\u001B[38;5;241m=\u001B[39mtolerance)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'title'"
     ]
    }
   ],
   "source": [
    "reddit_df['body'] = reddit_df['body'].replace('\\n', ' ').replace('\\r', '')\n",
    "for i in reddit_df:\n",
    "    reddit_df['body'][i] = reddit_df['body'][i].strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "#reddit_df.to_csv(\"wallstreetbets.csv\", index = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upload Reddit threads to MongoDB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DAPproject-twitter', 'admin', 'local']\n"
     ]
    }
   ],
   "source": [
    "# Connect to MongoDB Atlas\n",
    "\n",
    "# pip install pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "cluster = \"mongodb+srv://markdata:DAPproject@dapcluster.hbqohv5.mongodb.net/?retryWrites=true&w=majority\"\n",
    "# user = 'markdata'\n",
    "# password = 'DAPproject'\n",
    "client =  MongoClient(cluster)\n",
    "\n",
    "# Check database names\n",
    "print(client.list_database_names())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reddit_comments', 'DAPproject-twitter', 'reddit_threads', 'tweets']\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "db = client[\"DAPproject-twitter\"]\n",
    "print(db.list_collection_names())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert Reddit threads dataframe to dictionary and send to MongoDB collection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "<pymongo.results.InsertManyResult at 0x1a6bd5ed040>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop threads collection and recreate\n",
    "\n",
    "db.reddit_threads.drop()\n",
    "db.reddit_threads.insert_many(reddit_df.apply(lambda x: x.to_dict(), axis=1).to_list())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check number of documents\n",
    "db.reddit_threads.count_documents({})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterate over threads collection to IDs and then extract all comments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "thread_id = pd.DataFrame(db.reddit_threads.find().distinct('id'), columns=['id'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twiljd', 'txaa3y', 'u0u1io', 'u0yuy6', 'u1lvu9', 'u3e77l', 'u3g4fh', 'u3gp7z', 'u3jmb8', 'u4bvwa', 'u4zce5', 'u55a1k', 'u5xaf5', 'u8o6ds', 'u8rvwz', 'ub0j0g', 'ub8b15', 'ubbhtc', 'ubfi2o', 'ubiymk', 'ubp380', 'ubum21', 'ubw0dz', 'uc0cf9', 'ueg9jl', 'ueqe51', 'ufmieg', 'ukp7o9', 'umpes6', 'unyxol', 'uoplgm', 'upganz', 'uryudo', 'usruew', 'uuc104', 'uy509e', 'uz2hcx', 'v018sr', 'v66p37', 'vg9g5a', 'vtxya7', 'vu8ept', 'vulxrv', 'vummio', 'vun0fc', 'vuptn5', 'vuspuy', 'vutujn', 'vv1rgq', 'vv4469', 'vvct0j', 'vvrbz5', 'vw6uqs', 'vwa1mb', 'vxb7tw', 'vxjrnj', 'vxm9i3', 'vxpd8m', 'vy44bp', 'vyb52r', 'vywhxg', 'w08vx2', 'w0dyel', 'w30oqh', 'w33gju', 'w4rewy', 'w7viaj', 'wj9ach', 'wjm3r4', 'wv1nem', 'wv25nd', 'wvp80d', 'x11lsb', 'x1jcfn', 'x8598g', 'x86jpb', 'x9bpnz', 'xde8la', 'xf4yns', 'xs33ut', 'xvjbyz', 'xvjroy', 'xvtkso', 'xvxazg', 'xw7e8c', 'xwf00w', 'xwpv5h', 'xwsiyy', 'y904zo', 'y9f6f8', 'y9ndlg', 'y9urdr', 'yd4tft', 'ydzj7f', 'ye6cdp', 'yf94u0', 'yf9o79', 'yf9puh', 'yfbcup', 'yfnwds', 'yfz64h', 'ygrtih', 'yifz05', 'ynk8ba', 'yru8mm', 'z2uvbu']\n"
     ]
    }
   ],
   "source": [
    "print(thread_id.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "comments_df = []\n",
    "for n, name in enumerate(thread_id):\n",
    "    submission = reddit1.submission(id=thread_id[n])\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        comments_df.append([comment.parent_id, comment.body, comment.author, comment.score, comment.created_utc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# Convert list to DataFrame\n",
    "commentsdf = pd.DataFrame(comments_df, columns=['parent_id','Comment','author', 'comment_score', 'time_created'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   parent_id                                            Comment  \\\n",
      "0  t3_twiljd  \\n**User Report**| | | |\\n:--|:--|:--|:--\\n**T...   \n",
      "1  t3_twiljd              He's not, he's trolling Michael Burry   \n",
      "2  t3_twiljd  I don’t think he will buy. He only bought enou...   \n",
      "3  t3_twiljd  Please Elon, Don’t talk shit while I am holdin...   \n",
      "4  t3_twiljd  The rich need to influence popular belief. Bez...   \n",
      "\n",
      "                 author  comment_score  time_created  \n",
      "0             VisualMod              1  1.649123e+09  \n",
      "1  Objective_Letter_964             68  1.649122e+09  \n",
      "2      DisconnectedDays            149  1.649121e+09  \n",
      "3         Milot25wallst             27  1.649121e+09  \n",
      "4     grizzleSbearliano             84  1.649121e+09  \n"
     ]
    }
   ],
   "source": [
    "print(commentsdf.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "commentsdf['time_created'] = pd.to_datetime(commentsdf['time_created'], unit='s')\n",
    "commentsdf = commentsdf.drop(index=0)\n",
    "commentsdf.index = range(len(commentsdf.index))\n",
    "# Author column giving errors when converting so dropping\n",
    "commentsdf = commentsdf.drop(['author'], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   parent_id                                            Comment  \\\n",
      "0  t3_twiljd              He's not, he's trolling Michael Burry   \n",
      "1  t3_twiljd  I don’t think he will buy. He only bought enou...   \n",
      "2  t3_twiljd  Please Elon, Don’t talk shit while I am holdin...   \n",
      "3  t3_twiljd  The rich need to influence popular belief. Bez...   \n",
      "4  t3_twiljd  How about Twitter bans anyone that tries to ge...   \n",
      "\n",
      "   comment_score        time_created  \n",
      "0             68 2022-04-05 01:26:29  \n",
      "1            149 2022-04-05 01:14:13  \n",
      "2             27 2022-04-05 01:15:21  \n",
      "3             84 2022-04-05 01:06:59  \n",
      "4             22 2022-04-05 02:26:25  \n"
     ]
    }
   ],
   "source": [
    "print(commentsdf.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "<pymongo.results.InsertManyResult at 0x1a6c451ca30>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop comments collection and recreate\n",
    "\n",
    "db.reddit_comments.drop()\n",
    "db.reddit_comments.insert_many(commentsdf.apply(lambda x: x.to_dict(), axis=1).to_list())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract Twitter data next"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Twitter scraper\n",
    "# Oringinal source: https://medium.com/machine-learning-mastery/how-to-scrape-millions-of-tweets-using-snscraper-aa47cee400ec\n",
    "# Edited by Mark\n",
    "\n",
    "# Import packages\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Set max number of tweets\n",
    "maxTweets = 10000\n",
    "# Creating list to append tweet data to\n",
    "tweets_list2 = []\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "users = [\"elonmusk\"]\n",
    "for j, user in enumerate(users):\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:{} since:2021-12-01'.format(users[j])).get_items()):\n",
    "        if i>maxTweets:\n",
    "            break\n",
    "        tweets_list2.append([ tweet.id, tweet.date, tweet.rawContent, tweet.user.username, tweet.quoteCount, tweet.likeCount, tweet.replyCount, tweet.retweetCount, tweet.inReplyToTweetId, tweet.mentionedUsers])\n",
    "\n",
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['tweet_id','datetime', 'text', 'username', 'quotecount', 'likecount', 'replycount', 'retweetcount', 'tweetinreply', 'mentionedusers'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              tweet_id                  datetime  \\\n",
      "0  1601668368737906688 2022-12-10 15:01:11-05:00   \n",
      "1  1601667312930590721 2022-12-10 14:56:59-05:00   \n",
      "2  1601660414743687169 2022-12-10 14:29:34-05:00   \n",
      "3  1601658384947638272 2022-12-10 14:21:31-05:00   \n",
      "4  1601625011009179649 2022-12-10 12:08:54-05:00   \n",
      "\n",
      "                                                text  username  quotecount  \\\n",
      "0                            https://t.co/80DdvpsNjM  elonmusk        1291   \n",
      "1  Twitter is both a social media company and a c...  elonmusk        3008   \n",
      "2  @elizableu Looks like Yoel is arguing in favor...  elonmusk        1100   \n",
      "3                     @elizableu This explains a lot  elonmusk          90   \n",
      "4                            https://t.co/xKmQ8m7uHq  elonmusk         141   \n",
      "\n",
      "   likecount  replycount  retweetcount  tweetinreply  \\\n",
      "0     154727        7896         18082           NaN   \n",
      "1     174649       13154         27178           NaN   \n",
      "2      20465        2556          5630  1.601658e+18   \n",
      "3      29890         850          2693  1.601636e+18   \n",
      "4      44830        1663          2938  1.601625e+18   \n",
      "\n",
      "                    mentionedusers  \n",
      "0                             None  \n",
      "1                             None  \n",
      "2  [https://twitter.com/elizableu]  \n",
      "3  [https://twitter.com/elizableu]  \n",
      "4                             None  \n"
     ]
    }
   ],
   "source": [
    "print(tweets_df2.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# Convert Epoch time to datetome of New York timezone to correspond to stock prices\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "tweets_df2['datetime'] = pd.to_datetime(tweets_df2['datetime'], unit='s').dt.tz_convert('America/New_York')\n",
    "tweets_df2 = tweets_df2.drop(['mentionedusers'], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4700\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets_df2))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# Save tweets as JSON and upload to MongoDB\n",
    "tweets_df2.reset_index().to_json('twitter.json',orient='records', date_format='iso', indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "with open('twitter.json') as file:\n",
    "    file_data = json.load(file)\n",
    "#print(file_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "<pymongo.results.InsertManyResult at 0x1a6c5bced00>"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check if collection exists already and drop it if it does\n",
    "\n",
    "db.tweets.drop()\n",
    "# Attach JSON to collection\n",
    "db.tweets.insert_many(file_data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
