{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reddit scraper\n",
    "# Original source: https://www.geeksforgeeks.org/scraping-reddit-using-python/\n",
    "# Edited by Mark\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connect to Reddit API using PRAW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_client_id = '633UnblFcVgTtdNNS-hQUA'\n",
    "my_client_secret = 'h2phjjUTg5EkhNqob_PV5-2BU7iENA'\n",
    "my_user_agent = \"DAP_scrape\"\n",
    "#auth = requests.auth.HTTPBasicAuth(app_id, secret)\n",
    "reddit_username = 'McNData'\n",
    "reddit_password = 'NJdVr7hDXsAfgwEbUnfs'\n",
    "\n",
    "reddit1 = praw.Reddit(\n",
    "    client_id=\"633UnblFcVgTtdNNS-hQUA\",\n",
    "    client_secret=\"h2phjjUTg5EkhNqob_PV5-2BU7iENA\",\n",
    "    password=\"NJdVr7hDXsAfgwEbUnfs\",\n",
    "    user_agent=\"DAP_scrape\",\n",
    "    username=\"McNData\")\n",
    "subred = \"WallStreetBets\"\n",
    "subreddit = reddit1.subreddit(subred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scrape WallStreetBets subreddit thread titles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "import re\n",
    "# Display the name of the Subreddit\n",
    "#print(\"Display Name:\", subreddit.display_name)\n",
    "\n",
    "# Display the title of the Subreddit\n",
    "#print(\"Title:\", subreddit.title)\n",
    "\n",
    "# Display the description of the Subreddit\n",
    "#print(\"Description:\", subreddit.description)\n",
    "\n",
    "\n",
    "#subreddit = reddit1.subreddit(\"WallStreetBets\")\n",
    "\n",
    "#for post in subreddit.hot(limit=100):\n",
    " #   title = post.title.encode('utf-8')\n",
    "  #  print(title)\n",
    "   # print()\n",
    "\n",
    "\n",
    "posts = reddit1.subreddit(\"wallstreetbets\")\n",
    "# Scraping the top posts of the current month\n",
    "\n",
    "posts_dict = {\"Title\": [], \"Post Text\": [],\n",
    "              \"ID\": [], \"Score\": [],\n",
    "              \"Total Comments\": [] #\"Post URL\": []\n",
    "              }\n",
    "posts_data = []\n",
    "for post in posts.search((\"elon musk\"), limit=1000):\n",
    "    #if \"twitter\" in post.title:\n",
    "        # Title of each post\n",
    "    posts_data.append([post.title,int(post.created), post.selftext,post.id,post.score,post.num_comments])\n",
    "\n",
    "\n",
    "# Convert Epoch time to datetime\n",
    "reddit_df = pd.DataFrame(posts_data, columns = ['title','created','body', 'id','score','num_comments'])\n",
    "# Filter for posts only with Twitter in title\n",
    "reddit_df = reddit_df[reddit_df[\"title\"].str.contains(\"Twitter\") == True]\n",
    "\n",
    "\n",
    "reddit_df['created'] = pd.to_datetime(reddit_df['created'], unit='s')\n",
    "# reset the index\n",
    "reddit_df = reddit_df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# Convert Epoch time in comments to UTC datetime\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "\n",
    "reddit_df['created'] = pd.to_datetime(reddit_df['created'], unit='s')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "#reddit_df.to_csv(\"wallstreetbets.csv\", index = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upload Reddit threads to MongoDB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Connect to MongoDB Atlas\n",
    "\n",
    "# pip install pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "cluster = \"mongodb+srv://markdata:DAPproject@dapcluster.hbqohv5.mongodb.net/?retryWrites=true&w=majority\"\n",
    "# user = 'markdata'\n",
    "# password = 'DAPproject'\n",
    "client =  MongoClient(cluster)\n",
    "\n",
    "# Check database names\n",
    "print(client.list_database_names())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "db = client[\"DAPproject-twitter\"]\n",
    "print(db.list_collection_names())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert Reddit threads dataframe to dictionary and send to MongoDB collection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "<pymongo.results.InsertManyResult at 0x1cca1f43430>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop threads collection and recreate\n",
    "\n",
    "db.reddit_threads.drop()\n",
    "db.reddit_threads.insert_many(reddit_df.apply(lambda x: x.to_dict(), axis=1).to_list())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "106"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of documents\n",
    "db.reddit_threads.count_documents({})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterate over threads collection to IDs and then extract all comments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "thread_id = db.reddit_threads.find().distinct('id')\n",
    "for name in thread_id:\n",
    "    submission = reddit1.submission(id=reddit_df['id'][i])\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        comments_df.append([comment.body, comment.author, comment.score, comment.parent_id, comment.created_utc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "# Convert list to DataFrame\n",
    "commentsdf = pd.DataFrame(comments_df, columns=['Comment','author', 'comment_score', 'parent_id','time_created'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29190\n"
     ]
    }
   ],
   "source": [
    "print(len(commentsdf))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "commentsdf['time_created'] = pd.to_datetime(commentsdf['time_created'], unit='s')\n",
    "commentsdf = commentsdf.drop(index=0)\n",
    "commentsdf.index = range(len(commentsdf.index))\n",
    "# Author column giving errors when converting so dropping\n",
    "commentsdf = commentsdf.drop(['author'], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "print(commentsdf.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "<pymongo.results.InsertManyResult at 0x1cca56bd250>"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop comments collection and recreate\n",
    "\n",
    "db.reddit_comments.drop()\n",
    "db.reddit_comments.insert_many(commentsdf.apply(lambda x: x.to_dict(), axis=1).to_list())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract Twitter data next"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "# Twitter scraper\n",
    "# Oringinal source: https://medium.com/machine-learning-mastery/how-to-scrape-millions-of-tweets-using-snscraper-aa47cee400ec\n",
    "# Edited by Mark\n",
    "\n",
    "# Import packages\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Set max number of tweets\n",
    "maxTweets = 10000\n",
    "# Creating list to append tweet data to\n",
    "tweets_list2 = []\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "users = [\"elonmusk\"]\n",
    "for j, user in enumerate(users):\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:{} since:2022-03-01 until:2022-11-11'.format(users[j])).get_items()):\n",
    "        if i>maxTweets:\n",
    "            break\n",
    "        tweets_list2.append([ tweet.id, tweet.date, tweet.rawContent, tweet.user.username, tweet.quoteCount, tweet.likeCount, tweet.replyCount, tweet.retweetCount, tweet.inReplyToTweetId, tweet.mentionedUsers])\n",
    "\n",
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['tweet_id','datetime', 'text', 'username', 'quotecount', 'likecount', 'replycount', 'retweetcount', 'tweetinreply', 'mentionedusers'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert Epoch time to datetome of New York timezone to correspond to stock prices\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "tweets_df2['datetime'] = pd.to_datetime(tweets_df2['datetime'], unit='s').dt.tz_convert('America/New_York')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "print(len(tweets_df2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark1\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:224: FutureWarning: description is deprecated, use renderedDescription instead\n",
      "  return dumps(\n",
      "C:\\Users\\mark1\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:224: FutureWarning: descriptionUrls is deprecated, use descriptionLinks instead\n",
      "  return dumps(\n",
      "C:\\Users\\mark1\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:224: FutureWarning: linkTcourl is deprecated, use link.tcourl instead\n",
      "  return dumps(\n",
      "C:\\Users\\mark1\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:224: FutureWarning: linkUrl is deprecated, use link.url instead\n",
      "  return dumps(\n"
     ]
    }
   ],
   "source": [
    "# Save tweets as JSON and upload to MongoDB\n",
    "tweets_df2.reset_index().to_json('twitter.json',orient='records', date_format='iso', indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "with open('twitter.json') as file:\n",
    "    file_data = json.load(file)\n",
    "#print(file_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "<pymongo.results.InsertManyResult at 0x1cca5dc58e0>"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check if collection exists already and drop it if it does\n",
    "\n",
    "db.tweets.drop()\n",
    "# Attach JSON to collection\n",
    "db.tweets.insert_many(file_data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
